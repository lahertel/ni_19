{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9: Principal and Independent Component Analysis (PCA & ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.1: Principal Component Analysis (PCA)\n",
    "Principal component analysis (PCA) is a procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{L \\times d}$ be a given data matrix that contains $L$ `samples` $\\vec{x}_i \\in \\mathbb{R}^d$, $i = 1,...,L$.\n",
    "The principal components of $X$ can be derived by computing the `eigenvectors` $W = (\\vec{w}_1, ..., \\vec{w}_d)$ of the sample covariance matrix\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    C_X = \\frac{1}{L-1} \\sum_{i=1}^{L} \\left(\\vec{x}_{i} - \\vec{\\mu}_{X}\\right) \\left( \\vec{x}_{i} - \\vec{\\mu}_{X} \\right)^{T}\n",
    "\\end{equation},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\vec{\\mu}_{X} = \\frac{1}{L}\\sum_{i=1}^{L} \\vec{x}_{i}\n",
    "\\end{equation}.\n",
    "$$\n",
    "\n",
    "1. Implement the function `pca(samples)`, that computes the `eigenvectors` $W$ and the `eigenvalues` of $C_X$. Sort the `eigenvalues` and `eigenvectors` such that the ith column of $W$ corresponds to the ith largest `eigenvalue`.\n",
    "2. Apply your function to the `samples` in file `data_9_1.npz`.\n",
    "3. Use the function `utils.plot_principal_components` to visualize the principal components on the original data cloud.\n",
    "4. Perform a _change of basis_, i.e. represent the `samples` $X$ within the obtained (`eigenvectors`) basis $W$ (take care to subtract the sample mean $\\vec{\\mu}_{X}$ from each sample $\\vec{x}_{i}$ beforehand) and plot the result using `utils.plot_data`.\n",
    "5. Do you observe a connection between the `eigenvalues` and the marginal sample variances after the transformation?\n",
    "6. How does the covariance matrix before and after the transformation look like?\n",
    "\n",
    "__Hint__: The Python functions `np.cov`, `np.linalg.eig` and `np.argsort` might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import utils_9 as utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(samples):\n",
    "    # TODO: calculate the covariance matrix\n",
    "    covariance = \n",
    "    \n",
    "    # TODO: calculate the eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = \n",
    "    \n",
    "    # TODO: sort the eigenvectors and eigenvalues in descending order\n",
    "    \n",
    "    return (eigenvalues, eigenvectors)\n",
    "\n",
    "samples = utils.load_data('data/data_9_1.npz')\n",
    "eigenvalues, eigenvectors = pca(samples)\n",
    "utils.plot_principal_components(samples, eigenvectors)\n",
    "\n",
    "# TODO: perform a change of basis\n",
    "samples_pca = \n",
    "utils.plot_data(samples_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.2: Independent Component Analysis (ICA)\n",
    "This exercise considers the Independent Component Analysis (ICA). On the website, you find the file `data_9_2.npz`. This file contains 10,000 `samples` of a two-dimensional data distribution. Furthermore, you also find the Python function `utils.plot_joint(samples)`, which can be used to plot the marginal distributions of the data (i.e., the distribution of the first and second component of the data). Here, we want to manually perform ICA by implementing the following steps:\n",
    "\n",
    "1. Load the `samples` and plot the marginal distributions using `utils.plot_joint`.\n",
    "2. Compute and subtract the mean of the `samples`.\n",
    "3. Remove correlations from the data by performing a `pca`. What does the data look like after this step has been applied?\n",
    "4. Now, correlations have been removed from the data but the dimensions do not have the same variance. In order to align the variances of the dimensions, each dimension is divided by its standard deviation. This step is also called _whitening_. Apply this whitening step to the data and visualize the data after this step has been performed. __Note__: The `pca` function also provides the eigenvalues of the covariance matrix of the data which correspond to the variances of the data dimensions after the PCA has been applied.\n",
    "5. The last step tries to maximize the statistical independence of the data dimensions by rotating the whitened data as follows: $Y = X A_{\\theta}^{T}$, where\n",
    "\n",
    " $$\n",
    "\\begin{equation}\n",
    "    A_{\\theta} = \\left(\n",
    "        \\begin{matrix}\n",
    "            \\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "            \\sin(\\theta)  & \\cos(\\theta)\n",
    "        \\end{matrix}\n",
    "    \\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " is a rotation matrix w.r.t. an angle $\\theta$, such that the non-Gaussianity of the marginal data distributions is\n",
    "maximal, i.e., until the marginal distributions are as dissimilar to a Gaussian distribution as\n",
    "possible. Utilize the kurtosis to measure the non-Gaussianity, i.e., (iteratively) find the rotation\n",
    "\n",
    " $$\n",
    "\\begin{equation}\n",
    "    \\theta^{*} = \\underset{\\theta}{\\operatorname{argmax}}\\left(\n",
    "        \\left\\vert \n",
    "            \\frac{1}{2} \\left(\\text{kurtosis}(y_1) + \\text{kurtosis}(y_2) \\right) - 3 \n",
    "        \\right\\vert\n",
    "    \\right)\n",
    "\\end{equation}.\n",
    "$$\n",
    " Again, visualize your result.\n",
    "6. Why do we look for the rotation that leads to maximal non-Gaussianity in the resulting data dimensions?\n",
    "\n",
    "__Hint__: The Python functions `utils.rotation_matrix` and `utils.kurtosis` might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot the original data and its marginal distributions\n",
    "samples = utils.load_data('data/data_9_2.npz')\n",
    "utils.plot_joint(samples, \n",
    "                 title='Original Data')\n",
    "\n",
    "# TODO: Perform a PCA and plot the data\n",
    "samples_pca = \n",
    "utils.plot_joint(samples_pca, \n",
    "                 title='After PCA')\n",
    "\n",
    "# TODO: Whiten and plot the data\n",
    "samples_whitened = \n",
    "utils.plot_joint(samples_whitened, \n",
    "                 title='After Whitening')\n",
    "\n",
    "# TODO: Perform a manual ICA (find the optimal rotation angle) and plot the data\n",
    "samples_ica = \n",
    "utils.plot_joint(samples_ica, \n",
    "                 title='After ICA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
