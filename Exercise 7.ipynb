{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs in PyTorch\n",
    "PyTorch provides efficient implementations of a number of recurrent architectures, including the most prominent ones Gated Recurrent Units (GRUs) and Long Short Term Memory (LSTMs). \n",
    "\n",
    "While it is possible to build recurrent neural network architectures from basic tensor operations, it is usually much slower due to additional overhead and the sequential nature of RNNs. \n",
    "\n",
    "### Language modelling\n",
    "Language models are a semi-supervised approach where a model receives a partial sequence of characters (in character-level language models) or words (in word-level language models) as input and is expected to predict the next character/word token in the sequence. \n",
    "\n",
    "In this exercise we will train a word-level language model on the PTB dataset using RNNs.\n",
    "\n",
    "### The Penn Tree Bank (PTB) dataset\n",
    "The PTB dataset consists of sentences from newspaper articles. Numberals are replaced by just the capital letter \"N\" and any words outside the 10.000 most frequent ones are replaced with the \"<unk>\" token. This way all words are guaranteed to be frequent enough to allow a language model to generalize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the PTB dataset\n",
    "from utils.utils_7 import PTB\n",
    "\n",
    "print('Loading dataset...')\n",
    "    \n",
    "ptb_train = PTB(\"data/ptb.train.txt\")#training set\n",
    "ptb_valid = PTB(\"data/ptb.valid.txt\")#validation set\n",
    "ptb_test = PTB(\"data/ptb.test.txt\")#test set\n",
    "\n",
    "#determine the full set of words\n",
    "word_set = ptb_train.word_set.union(ptb_valid.word_set, ptb_test.word_set)\n",
    "\n",
    "dictionary = {\"<padding>\": 0}#dictionary to form network inputs from words\n",
    "inv_dictionary = {0: \"<padding>\"}#inverse dictionary to retrieve the actual words from network outputs\n",
    "#assign a dictionary index to every word\n",
    "for i, word in enumerate(word_set):\n",
    "    dictionary[word] = i+1\n",
    "    inv_dictionary[i+1] = word\n",
    "\n",
    "ptb_train.encode_sentences(dictionary)\n",
    "ptb_valid.encode_sentences(dictionary)\n",
    "ptb_test.encode_sentences(dictionary)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Padding of sequences\n",
    "RNNs are often used for sequence data like audio waves, texts in the form of character sequences or word sequences, or any type of time series. \n",
    "\n",
    "Invoking the same operations multiple times (once per sequence rather than once per batch of sequences) introduces a lot of overhead. But since sequences can vary in length, it's not possible to just stack them to form a batch tensor, so we need to pad the sequences to make them the same length. \n",
    "\n",
    "The PTB class instantiated above is a Dataset object. It handles loading of single samples from the dataset but is not responsible for padding, batching or shuffling - that's the job of the `DataLoader`.\n",
    "\n",
    "The `DataLoader` selects sample indices for a batch, retrieves the corresponding samples from the dataset and calls the `collate` function to combine the list of samples into a batch. Implement the collate function for the dataloaders below.\n",
    "\n",
    "__Programming Hints__:\n",
    " - Use `torch.nn.utils.rnn.pad_sequence` for padding and use a padding value of `0` as defined in the dictionary above.\n",
    " - Depending on your choice `pad_sequence` will either use the first tensor dimension of its result tensor for the sequence position and the second dimension for the batch index or vice versa. This will be important to remember further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def pad_and_batch_sequences(sequences):\n",
    "    #sequences is a list of 1-dimensional tensors of different size\n",
    "    #each tensor represents a sentence\n",
    "    #each element of a tensor is the index of a word in the dictionary\n",
    "    \n",
    "    # TODO: sort the list of sequences by DESCENDING length (required in PyTorch 1.0, possibly unnecessary in later versions)\n",
    "    sequences...\n",
    "    \n",
    "    # TODO: pad the sequences and combine them into a batch\n",
    "    batch = \n",
    "    \n",
    "    # TODO: calculate the original lengths of the sequences and store them in a tensor\n",
    "    lengths = \n",
    "    \n",
    "    return batch, lengths\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=ptb_train,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 collate_fn=pad_and_batch_sequences)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "                dataset=ptb_valid,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=pad_and_batch_sequences)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=ptb_test,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=pad_and_batch_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.2: Building the model\n",
    "Let's start with a simple model with just a recurrent module and a linear layer to generate the output scores for the dictionary entries. \n",
    "\n",
    "It has proven advantageous to represent every entry of the dictionary by a vector, called embedding. The values of the vectors can start out randomly and be optimized through backpropagation during training. \n",
    "\n",
    "__Preparation__:\n",
    " - Choose a type of recurrent layer.\n",
    " - Get familiar with `torch.nn.utils.rnn.pack_padded_sequence` to create `PackedSequence` objects. Let the recurrent module work on `PackedSequence` objects rather than Tensors.\n",
    " \n",
    "__Programming Hints__:\n",
    " - Use the PyTorch module `torch.nn.Embedding` for the word embeddings. An instance of `torch.nn.Embedding` takes a tensor of indices and returns a tensor where every entry is replaced with the corresponding embedding-vector (so the result tensor also has one additional dimension of size `embedding-dim`). \n",
    " - Let the recurrent module have `2` layers.\n",
    " - `Packing` allows you to pack the padded batch tensor tightly and to let the recurrent module know how long each of the sequences really are, so it doesn't need to calculate the outputs for the padding as well. The recurrent module can either take a `Tensor` and return its outputs and final hidden state as `Tensor` objects, or take a `PackedSequence` in which case the outputs object will also be a `PackedSequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        embedding_dim = 1000\n",
    "        hidden_size = 1000\n",
    "        \n",
    "        # TODO: instanciate an Embedding module\n",
    "        self.embedding = \n",
    "        \n",
    "        # TODO: give the model a recurrent module with 2 layers\n",
    "        self.rnn = \n",
    "        \n",
    "        # TODO: create a linear layer to generate the output that scores each entry of the dictionary\n",
    "        self.fc = \n",
    "        \n",
    "    def forward(self, batch, lengths):\n",
    "        # TODO: generate the tensor of embedding vectors from the tensor of word indices\n",
    "        batch_embedded = \n",
    "        \n",
    "        # TODO: pack the sequences tightly\n",
    "        batch_embedded_packed = \n",
    "        \n",
    "        # TODO: apply the RNN\n",
    "        outputs, _ = \n",
    "        \n",
    "        # TODO: unpack the results by transforming them into a padded tensor again (pad_packed_sequence)\n",
    "        outputs_padded, _ = \n",
    "        \n",
    "        # TODO: apply the linear layer\n",
    "        predictions = \n",
    "        \n",
    "        return predictions\n",
    "###\n",
    "# Hyperparameters:\n",
    "###\n",
    "    \n",
    "#The starting learning rate\n",
    "lr=0.007\n",
    "\n",
    "#The factor by which the learning rate is decreased after each epoch\n",
    "lr_decay=0.6\n",
    "\n",
    "#The smallest value the learning rate can decay to\n",
    "lr_min=5e-4\n",
    "\n",
    "#The importance of the L2-regularization term - not actually weight decay for Adam optimizer implementation but still using the same name since it is equivalent to actual weight decay in the vanilla SGD optimizer\n",
    "weight_decay = 8e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.3: Training the model\n",
    "For each sample sequence we can use the first word to predict the second, use the first two words to predict the third, use the first three words to predict the fourth, and so on. Each prediction is a classification problem where the number of classes is the dictionary size. The elements of the input sequence are the labels (except for the first element, since there is no input to predict it) and the outputs are the predicted classes (except for the last output, which goes beyond the last label). \n",
    "\n",
    "For each predicted word we can calculate the cross entropy loss and average it over all the predictions of a sequence and over all the sequences of a batch. Implement the training and evaluation functions.\n",
    "\n",
    "__Preparation__:\n",
    " - Think carefully about which part of the input sequences and the network outputs can be used for the loss calculation and how they align.\n",
    "\n",
    "__Programming Hints__:\n",
    " - The loss function is set up to ignore predictions where the label is the padding value (see `ignore_index` further down), so the loss function can take padded tensors.\n",
    " - `Perplexity` is commonly used to evaluate a language model's performance. Roughly speaking it tells us how many words the model considers as candidates per predicion (on average). So for a completely untrained model with a dictionary size of 10.000 we would expect a perplexity of 10.000 and for a perfect model we would expect a perplexity of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, use_gpu, optimizer, loss_func):\n",
    "    model.train()\n",
    "    for i, (batch, lengths) in enumerate(dataloader):\n",
    "        if use_gpu:\n",
    "            #move the batch to gpu memory\n",
    "            batch = batch.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TODO: get the predictions\n",
    "        out = \n",
    "        \n",
    "        # TODO: get the part of the batch that should be used as labels\n",
    "        targets = \n",
    "        \n",
    "        # TODO: calculate the loss\n",
    "        loss = \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i % 100 == 0 and use_gpu) or (i % 5 == 0 and not use_gpu):\n",
    "            perplexity = loss.exp()\n",
    "            print(perplexity)\n",
    "            \n",
    "def evaluate(model, dataloader, use_gpu, optimizer, loss_func):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, (batch, lengths) in enumerate(dataloader):\n",
    "            if use_gpu:\n",
    "                #move the batch to gpu memory\n",
    "                batch = batch.cuda()\n",
    "            \n",
    "            # TODO: get the predictions\n",
    "            out = \n",
    "            \n",
    "            # TODO: get the part of the batch that should be used as labels\n",
    "            targets = \n",
    "            \n",
    "            # TODO: calculate the loss\n",
    "            loss = \n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "        perplexity = torch.stack(losses, dim=0).mean().exp()\n",
    "\n",
    "        print('evaluation perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model. If everything is set up correctly, the perplexity should go well below 1000 within the first epoch.\n",
    "\n",
    "__Note__: The model starts overfitting after a few epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": model.parameters(), \"weight_decay\": weight_decay},\n",
    "    ], lr=lr)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=0)#ignore targets in the padding section (label=0)\n",
    "\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch \" + str(epoch))\n",
    "    \n",
    "    train(model, train_loader, use_gpu, optimizer, loss_func)\n",
    "    \n",
    "    evaluate(model, valid_loader, use_gpu, optimizer, loss_func)\n",
    "    \n",
    "    lr *= lr_decay\n",
    "    if lr < lr_min:\n",
    "        lr = lr_min\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    torch.save((model.state_dict(), dictionary), \"model\" + str(epoch) + \".pt\")\n",
    "        \n",
    "print(\"Test set perplexity:\")\n",
    "evaluate(model, test_loader, use_gpu, optimizer, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the language model\n",
    "Since this model takes a long time to train on a CPU we have also provided a pretrained model. \n",
    "\n",
    "You can use the textbox below to get predictions from either the pretrained model or your own one. Use `backspace` while the textbox is empty to remove word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_7 import get_trained_model, make_prediction_field\n",
    "\n",
    "use_pretrained = True\n",
    "\n",
    "if use_pretrained:\n",
    "    #to test the pretrained model:\n",
    "    trained_model, original_dictionary, original_inv_dictionary = get_trained_model()\n",
    "    batch_first = True #we used batch_first=true out of habit\n",
    "else:\n",
    "    load_weights = True\n",
    "    \n",
    "    if not load_weights:\n",
    "        #to test the model from this notebook:\n",
    "        trained_model, original_dictionary, original_inv_dictionary = model, dictionary, inv_dictionary\n",
    "    else:\n",
    "        #to test the model from this notebook from stored weights:\n",
    "        model_state_dict, original_dictionary = torch.load(\"model3.pt\", map_location=\"cpu\")\n",
    "        original_inv_dictionary = {v: k for k, v in original_dictionary.items()}\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        trained_model = model\n",
    "        \n",
    "    batch_first = False #False unless you explicitly specified batch_first=true in the packing, padding and RNN functions and implemented the loss calculation accordingly\n",
    "\n",
    "if use_gpu:\n",
    "    trained_model = trained_model.cuda()\n",
    "\n",
    "def predict_func(sentence):#sentence as list of word strings\n",
    "    #max number of additional words to predict\n",
    "    max_len = 50\n",
    "    for _ in range(max_len):\n",
    "        #create a tensor from the words' dictionary indices\n",
    "        input_sentence = torch.tensor([original_dictionary[word] for word in sentence])\n",
    "        \n",
    "        #introduce the singular batch dimension\n",
    "        if batch_first:\n",
    "            input_sentence = input_sentence.unsqueeze(dim=0)\n",
    "            lengths = torch.tensor([input_sentence.size()[1]])\n",
    "        else:\n",
    "            input_sentence = input_sentence.unsqueeze(dim=1)\n",
    "            lengths = torch.tensor([input_sentence.size()[0]])\n",
    "            \n",
    "        if use_gpu:\n",
    "            input_sentence = input_sentence.cuda()\n",
    "        \n",
    "        #use the language model to predict the most likely next word\n",
    "        trained_model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = trained_model(input_sentence, lengths)\n",
    "        \n",
    "        #ignore predictions of the placeholder for rare words (dictionary limited to the 10k most common words in the dataset)\n",
    "        if batch_first:\n",
    "            out[0, -1, original_dictionary['<unk>']] = 0\n",
    "            out = out.argmax(-1)[0,-1]\n",
    "        else:\n",
    "            out[-1, 0, original_dictionary['<unk>']] = 0\n",
    "            out = out.argmax(-1)[-1,0]\n",
    "        \n",
    "        #get the string representation of the predicted word\n",
    "        out_word = original_inv_dictionary[out.item()]\n",
    "        \n",
    "        #append word to sentence word list\n",
    "        sentence.append(out_word)\n",
    "        \n",
    "        #stop predictions if end-of-sentence was predicted\n",
    "        if out_word == '<eos>': break\n",
    "            \n",
    "    #return the sentence as a single string\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "#create the input field with autocomplete for the available dictionary (autocomplete only shows suggestions if the number of matching suggestions is not too large)\n",
    "make_prediction_field(original_dictionary, \"predict_func\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
