{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Multilayer Perceptron (MLP)\n",
    "In this exercise, we will study a multilayer perceptron (MLP) with one hidden layer (comprising $M$ hidden neurons) and a single output neuron.\n",
    "\n",
    "We obtain the output of the MLP through forward propagation as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\vec{v} &= \\sigma_{\\beta} \\left( \\vec{\\tilde{x}} {W_{\\text{hidden}}}  \\right) \\\\\n",
    "    y &= \\sigma_{\\beta} \\left( \\vec{\\tilde{v}} \\vec{\\tilde{w}}_{\\text{output}} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\vec{\\tilde{x}} = \\left(-1, x_1, ..., x_N \\right)$ is the extended `sample` $\\vec{x}$, $W_{\\text{hidden}}=(\\vec{\\tilde{w}}^{T}_{1}, \\dots, \\vec{\\tilde{w}}^{T}_{M})$ are the threshold extended `hidden_weights` of the hidden neurons, $\\vec{v} = \\left( v_1, \\dots, v_M \\right)$ are the $M$ outputs of the hidden neurons, $\\vec{\\tilde{v}} = \\left(-1, v_1,..., v_M \\right)$ is the extended hidden layer output vector, $\\vec{\\tilde{w}} = \\left(\\theta, w_1, \\dots, w_M \\right)$ are the threshold extended `output_weights` of the output neuron, and $\\sigma_{\\beta} \\left(\\cdot\\right) = \\text{tanh}\\left(\\frac{\\beta}{2}\\cdot\\right)$ is the `sigmoid` function.\n",
    "\n",
    "__Note__: The _threshold trick_ is applied, i.e. the threshold of each neuron is included as an additional _first_ component for each extended weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Implementation of an MLP\n",
    "Implement the forward propagation of the MLP. Your code should be flexible enough to allow an arbitrary $M \\in \\mathbb{N}^{+}$. The `hidden_weights` of the hidden neurons $W_{\\text{hidden}}$, the `output_weights` of the output neuron $\\vec{\\tilde{w}}$, and `beta` $\\beta$ should be passed to your function as parameters.\n",
    "\n",
    "__Note__: Try to avoid for-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import utils_3 as utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the sigmoid perceptron activation function\n",
    "def sigmoid(x, beta):\n",
    "    pass\n",
    "\n",
    "# TODO: implement function for the forward propagation\n",
    "def classify_mlp(samples, hidden_weights, output_weights, beta):\n",
    "\n",
    "    # TODO: n_samples: number of samples\n",
    "    n_samples =\n",
    "\n",
    "    # TODO: extend samples by '-1' column (threshold trick)\n",
    "    samples =\n",
    "\n",
    "    # TODO: compute outputs of the hidden neurons\n",
    "    hidden_outputs =\n",
    "\n",
    "    # TODO: extend hidden outputs by '-1' column (threshold trick)    \n",
    "    hidden_outputs =\n",
    "\n",
    "    # TODO: compute outputs of the output neurons\n",
    "    outputs =\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Classification with an MLP\n",
    "On the website, you will find a NumPy file `data_3.npz` containing the weights for an MLP. Each of the weight vectors contains a threshold as its first component. Load the weights from the data file and visualize the classification that you obtain for $1000$ points distributed randomly in the region $\\left[ -1,1 \\right] \\times \\left[ -1,1 \\right]$. What shape does the classification boundary of the whole MLP have? Experiment with different values of $\\beta$, including $3$, $5$, $20$, and $\\beta \\to \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights, output_weights = utils.load_data('data/data_3.npz')\n",
    "\n",
    "# TODO: uniformly create 2D random data in [-1,1] x [-1,1] (1000 x 2 matrix)\n",
    "samples =\n",
    "\n",
    "# TODO: define the beta value (sigmoid function of the MLP)\n",
    "beta =\n",
    "\n",
    "# TODO: assign each output value >= 0 the class prediction +1 and each output value < 0 the class prediction -1\n",
    "classifications = classify_mlp(samples, hidden_weights, output_weights, beta)\n",
    "classifications =\n",
    "\n",
    "utils.plot_data(samples, classifications)\n",
    "utils.plot_classlines(hidden_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
