{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: k-Means Clustering and Neural Gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "%matplotlib inline\n",
    "from utils import utils_8 as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8.1: Implementing and Testing the k-Means Algorithm\n",
    "In many cases, the points in a data set can be grouped into several clusters of points that lie close together; each cluster can be described by a single representative point, the cluster center. In the first part of this exercise, we will study the k-means algorithm, a simple algorithm that learns a representation of clusters.\n",
    "We have $N$ `samples` $\\vec{x_\\mu} \\in \\mathbb{R}^d$, $\\mu = 1,...,N$ and want to find $k$ `codebook_vectors` $\\vec{w_i} \\in \\mathbb{R}^d$, $i = 1,...,k$ that represent the clusters in the data. Each data point $\\vec{x_\\mu}$ is assigned to the codebook vector that has the smallest euclidean distance to it; we call the index of this codebook vector $i^∗$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    i^∗ := \\underset{i}{\\operatorname{argmin}} \\lVert\\vec{x_\\mu} − \\vec{w_i}\\rVert_2.\n",
    "\\end{align}\n",
    "$$\n",
    "Here is the pseudocode for the k-means algorithm: \n",
    "\n",
    "for $t=1$ to $t_{\\max}$\n",
    "> for $\\mu=$ np.random.permutation$(N)$\n",
    ">> - Find the closest codebook vector $\\vec{w_{i^*}}$ to the data point $\\vec{x_\\mu}$. \n",
    "- Update the codebook vector $\\vec{w_{i^*}}$ with $\\vec{w_{i^*}} = \\vec{w_{i^*}} + \\varepsilon_t(\\vec{x_\\mu} − \\vec{w_{i^*}})$.\n",
    "\n",
    "where $\\varepsilon_t$ is the `learning_rate` that exponentially decays from a large value $\\varepsilon_{start}$ to a small value $\\varepsilon_{end}$ as the number of performed learning epochs (i.e. $t$) increases:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\varepsilon_t = \\varepsilon_{start}\\left(\\frac{\\varepsilon_{end}}{\\varepsilon_{start}}\\right)^{t/tmax}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Implement the k-means algorithm in Python \n",
    "- On the website, you will find a NumPy file data_10.npz containing a two-dimensional data set. Test your implementation several times using this data. Vary the codebook size $k$. What is a good choice for $\\varepsilon_{start}$ and $\\varepsilon_{end}$?\n",
    "\n",
    "__Hints__:\n",
    " - to avoid a loop over the codebook vectors when computing the distance of $\\vec{x_\\mu}$ to each codebook vector you can use the principle of broadcasting and calculate the norm of each row of the matrix (`help(np.linalg.norm)`, important: set the axis parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement the kMeansCluster function\n",
    "def kMeansCluster(samples, k, numEpochs, epsilon_start, epsilon_end, plot_mode, rand_seed):\n",
    "    \n",
    "    # TODO n_samples: number of training samples / n_features: number of features\n",
    "    n_samples, n_features = \n",
    "    \n",
    "    np.random.seed(rand_seed)\n",
    "    # randomly initialize codebook_vectors\n",
    "    codebook_vectors = 10 * np.random.rand(k,n_features) - 5;\n",
    "    # show initialisation\n",
    "    yield (codebook_vectors.copy(), [])\n",
    "    \n",
    "    liste = []\n",
    "    for epoch in range(numEpochs):\n",
    "    \n",
    "        # TODO: set the current learning rate epsilon_t\n",
    "        learning_rate = \n",
    "        \n",
    "        print('starting epoch t={} [learning rate={:.3f}]...'.format(epoch, learning_rate))\n",
    "        \n",
    "        # TODO: generate randomly permuted index array\n",
    "        indexes = \n",
    "        \n",
    "        # iterate through all indexes in the index array\n",
    "        for index in indexes:\n",
    "            sample = samples[index]\n",
    "        \n",
    "            # TODO: implement the k-means learning rule\n",
    "            \n",
    "            \n",
    "            if plot_mode == 'sample_wise':\n",
    "                yield (codebook_vectors.copy(), sample.copy())\n",
    "        \n",
    "        if plot_mode == 'epoch_wise':\n",
    "            yield (codebook_vectors.copy(),[])\n",
    "            \n",
    "    print('finished.\\n')\n",
    "    pass\n",
    "\n",
    "# load the data\n",
    "samples = utils.load_data('data/data_8.npz')\n",
    "\n",
    "# TODO: set the parameters\n",
    "k = \n",
    "numEpochs = \n",
    "epsilon_start = \n",
    "epsilon_end = \n",
    "\n",
    "# choose one of 'epoch_wise', 'sample_wise', use 'sample_wise' only for numEpochs = 1\n",
    "plot_mode = 'epoch_wise'\n",
    "\n",
    "# give rand_seed a value to compare neural gas and kMeans\n",
    "rand_seed = None\n",
    "\n",
    "# call the function\n",
    "animation = utils.Animation(samples,k)\n",
    "codebook_vektors = list(kMeansCluster(samples, k, numEpochs, \n",
    "                                      epsilon_start, epsilon_end, \n",
    "                                      plot_mode, rand_seed))\n",
    "animation.play(codebook_vektors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8.2: Implementing and Testing the Neural Gas Algorithm\n",
    "The so-called “Neural Gas” algorithm is very similar to the above k-means algorithm, but for a given `sample` it adapts all`codebook_vectors` in a “soft-competitive” fashion (instead of “hard- competitively” changing only the winner $i^∗(\\vec{x_\\mu})$). For codebook adaptation we use a neighborhood function $\\lambda_t$ that determines how much close-by codebook vectors are attracted by the current `sample` $\\vec{x_\\mu}$. Just like $\\varepsilon_t$, Neural Gas cools the neighborhood radius $\\lambda_t$ down from a large value $\\lambda_{start}$ to a small value $\\lambda_{end}$.\n",
    "\n",
    " - Compute the `neighborhood_radius` $\\lambda_t$ in `epoch` $t$ with\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\lambda_t = \\lambda_{start}\\left(\\frac{\\lambda_{end}}{\\lambda_{start}}\\right)^{t/tmax}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " - Compute the `rank` $r_i(\\vec{x_\\mu})$ of codebook vector $\\vec{w_i}$ with\n",
    "$$\n",
    "\\begin{align}\n",
    "    r_i(\\vec{x_\\mu}) = |\\{ j\\ |\\  \\lVert\\vec{x_\\mu} − \\vec{w_j}\\rVert_2 < \\lVert\\vec{x_\\mu} − \\vec{w_i}\\rVert_2\\}|,\n",
    "\\end{align}\n",
    "$$ \n",
    "i.e. the number of codebook vectors $\\vec{w}_{j}$ that have a smaller euclidean distance to $\\vec{x}_{\\mu}$ than $\\vec{w}_{i}$.  \n",
    " - Now update all $k$ `codebook_vectors` $\\vec{w_i}$ with \n",
    " $$\n",
    "\\begin{align}\n",
    "\\vec{w_{i}} = \\vec{w_{i}} + \\varepsilon_t e^{\\frac{-r_i(\\vec{x_\\mu})}{\\lambda_t}}(\\vec{x_\\mu} − \\vec{w_{i}}).\n",
    "\\end{align}\n",
    "$$\n",
    " - Test the Neural Gas algorithm on the given data set several times. Vary the codebook size $k$.\n",
    " - What is a good choice for $\\lambda_{start}$ and $\\lambda_{end}$?\n",
    " - What differences do you notice between representations learned by k-means and Neural Gas?\n",
    "\n",
    "\n",
    "__Hints__:\n",
    " - to avoid a loop over the codebook vectors when computing the distance of $\\vec{x_\\mu}$ to each codebook vector you can use the principle of broadcasting and calculate the norm of each row of the matrix (`help(np.linalg.norm)`, important: set the axis parameter)\n",
    " - the command `np.argsort` might help you to determine the rank of the codebook vectors and to accordingly select the closest, 2nd closest, etc. codebook vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the neuralGas function\n",
    "def neuralGas(samples, k, numEpochs, epsilon_start, epsilon_end, \n",
    "              lambda_start, lambda_end, plot_mode, rand_seed):\n",
    "    \n",
    "    # TODO n_samples: number of training samples / n_features: number of features\n",
    "    n_samples, n_features = \n",
    "    \n",
    "    np.random.seed(rand_seed)\n",
    "    # randomly initialize codebook_vectors\n",
    "    codebook_vectors = 10 * np.random.rand(k,n_features) -5;\n",
    "    \n",
    "    # show initialisation\n",
    "    yield (codebook_vectors.copy(), [])\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "    \n",
    "        # TODO: set the current learning rate and neighborhood radius\n",
    "        learning_rate = \n",
    "        neighborhood_radius = \n",
    "        \n",
    "        print('starting epoch t={} [learning rate={:.3f}, neighborhood radius = {:.3f}]...'.format(epoch, learning_rate, neighborhood_radius))\n",
    "    \n",
    "         # TODO: generate randomly permuted index array\n",
    "        indexes = \n",
    "        \n",
    "        # iterate through all indexes in the index array\n",
    "        for index in indexes:\n",
    "            sample = samples[index]\n",
    "            \n",
    "            # TODO: implement the Neural Gas learning rule\n",
    "              \n",
    "                \n",
    "            if plot_mode == 'sample_wise':\n",
    "                yield (codebook_vectors.copy(), sample.copy())\n",
    "        \n",
    "        if plot_mode == 'epoch_wise':\n",
    "            yield (codebook_vectors.copy(),[])\n",
    "\n",
    "    print('finished.\\n')\n",
    "    pass\n",
    "\n",
    "# load the data\n",
    "samples = utils.load_data('data/data_8.npz')\n",
    "\n",
    "# TODO: set the parameters\n",
    "k = \n",
    "numEpochs = \n",
    "epsilon_start = \n",
    "epsilon_end = \n",
    "lambda_start = \n",
    "lambda_end = \n",
    "\n",
    "# give rand_seed a value to compare neural gas and kMeans\n",
    "rand_seed = None\n",
    "\n",
    "# choose one of 'epoch_wise', 'sample_wise', use 'sample_wise' only for numEpochs = 1\n",
    "plot_mode = 'epoch_wise'\n",
    "\n",
    "# call the function\n",
    "animation = utils.Animation(samples,k)\n",
    "codebook_vektors = list(neuralGas(samples, k, numEpochs, \n",
    "                                      epsilon_start, epsilon_end, \n",
    "                                      lambda_start, lambda_end, \n",
    "                                      plot_mode ,rand_seed))\n",
    "animation.play(codebook_vektors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
