{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Perceptron Learning and Maximum Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Perceptron Learning\n",
    "Given $L$ training `samples` $\\vec{x}_i \\in \\mathbb{R}^{N}$ and its class `labels` $s_i\\in \\lbrace 1 , -1 \\rbrace$, we want to train a single artificial neuron, i.e. make it to automatically learn its `weights` $\\vec{w} \\in \\mathbb{R}^{N}$ and its `threshold` $\\theta \\in \\mathbb{R}$, such that\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\sigma \\left( \\vec{x}_{i} \\vec{w}- \\theta \\right) = s_{i}, \\; \\forall i=1,\\dots,L\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "holds. The sigmoid function is defined as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\sigma(x) = \n",
    "        \\left\\{ \n",
    "            \\begin{array}{rl}\n",
    "                1, & \\text{if } x \\geq 0 \\\\\n",
    "                -1, & \\text{else}\n",
    "            \\end{array} \n",
    "        \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The `weights` $\\vec{w}$ represents a normal vector of a linear hyperplane and `threshold` $\\theta$ represents its (by $\\lVert \\vec{w} \\rVert$ scaled) distance to the origin.\n",
    "\n",
    "To simplify learning, we apply the _threshold trick_, i.e. we extend the `weights` by an additional component in the first dimension that represents the `threshold` $\\theta$. The `samples` are likewise extended by a component with constant value $-1$ in the first dimension (see `help(np.column_stack)`). In this way, for an extended `sample` $\\vec{x} \\in \\mathbb{R}^{N+1}$ the output of the neuron can be written as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  y = \\sigma \\left( \\vec{x} \\vec{w} \\right)\n",
    "\\end{equation}.\n",
    "$$\n",
    "\n",
    "During each learning `epoch` the given $L$ training `samples` are presented to the artificial neuron in random order. We use the perceptron learning rule to adapt the extended `weights` $\\vec{w}_{t}$ to $\\vec{w}_{t+1}$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\vec{w}_{t+1} = \\vec{w}_{t} + \\varepsilon (s_{i} - y_{i}) \\vec{x}_{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_{i} = \\sigma \\left( \\vec{x}_{i} \\vec{w}_{t} \\right)\n",
    "\\end{equation}.\n",
    "$$\n",
    "\n",
    "Here, $\\varepsilon \\in \\mathbb{R}^{+}$ denotes the `learning_rate` and $\\vec{x}_{i}$ a randomly selected training sample.\n",
    "\n",
    "Implement the learning rule in Python. Furthermore, there are two data files `data_2_1.npz` and `data_2_2.npz` on the website that contain the training sets. Apply your perceptron implementation several times to the example training sets (start with a `learning_rate` $\\varepsilon=0.01$). Visualize the classification plane during the learning process.\n",
    "\n",
    "__Hints__:\n",
    "- In each `epoch` of the perceptron learning process a randomly selected training sample $\\vec{x}_i$ (see `help(np.random.permutation)`) with class label $y_{i}$ is classified. Then, the `weights` are modified according to the learning rule.\n",
    "- A single learning epoch might not be sufficient for obtaining a correct classification of all training samples. In this case, further learning epochs should be performed until a correct classification is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import utils_2 as utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_perceptron(samples, labels, learning_rate, epochs):\n",
    "\n",
    "    # TODO n_samples: number of training samples / n_features: number of features\n",
    "    n_samples, n_features =\n",
    "\n",
    "    # TODO: initialize extended weight vector (theta included) randomly\n",
    "    weights =\n",
    "\n",
    "    # TODO: extend features by '-1' column (threshold trick)\n",
    "    samples =\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        # TODO: generate randomly permuted index array\n",
    "        indexes =\n",
    "        \n",
    "        # iterate through all indexes in the index array\n",
    "        for index in indexes:\n",
    "\n",
    "            # TODO: select training sample and corresponding class label according to generated random permutation\n",
    "            sample =\n",
    "            label =\n",
    "        \n",
    "            # TODO: classify selected training sample with current weights\n",
    "            classification =\n",
    "        \n",
    "            # TODO: adapt weight vector, i.e. apply perceptron learning rule\n",
    "            weights =\n",
    "            \n",
    "            # yield weight vector and threshold\n",
    "            yield (weights[1:], weights[0])\n",
    "\n",
    "samples, labels = utils.load_data('data/data_2_1.npz')\n",
    "animation = utils.Animation(samples, labels)\n",
    "weights = list(learn_perceptron(samples, labels, \n",
    "                                learning_rate=0.01, \n",
    "                                epochs=2))\n",
    "animation.play(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: The DoubleMinOver Learning Rule\n",
    "From the lecture, you know that the DoubleMinOver (DMO) learning rule can be used for maximum margin classification. The DMO algorithm is summarized below. Note in particular that the `weights` $\\vec{w} \\in \\mathbb{R}^{N}$ (i.e. the _threshold trick_ is not applied) and that an explicit `threshold` $\\theta$ is used that is computed after learning has been completed.\n",
    "\n",
    "\n",
    "\n",
    "for $t=1$ to $t_{\\max}$\n",
    "> $\\vec{x}^{\\min +} = \\underset{\\vec{x}_i \\in X^{+}}{\\operatorname{argmin}}\n",
    "            s_{i} \\vec{x}_{i} \\vec{w}\n",
    "            \\left( X^{+} = \\left\\{ \\vec{x}_{i} \\mid s_{i} = 1 \\right\\} \\right)$  \n",
    "> $\\vec{x}^{\\min -} = \\underset{\\vec{x}_i \\in X^{-}}{\\operatorname{argmin}}\n",
    "            s_{i} \\vec{x}_{i} \\vec{w}\n",
    "            \\left( X^{-} = \\left\\{ \\vec{x}_{i} \\mid s_{i} = -1 \\right\\} \\right)$  \n",
    "> $\\vec{w} = \\vec{w} + \\vec{x}^{\\min +} - \\vec{x}^{\\min -}$\n",
    "\n",
    "$\\theta = \\frac{\\vec{w}^\\mathrm{T} \\left(\\vec{x}^{\\min +} + \\vec{x}^{\\min -}\\right)}{2}$\n",
    "\n",
    "- Implement the DoubleMinOver algorithm in Python.\n",
    "- Test your implementation on the two training data sets `data_2_1.npz` and `data_2_2.npz`.\n",
    "- Compare your DMO learning results with your perceptron learning results. Run both learning algorithms several times. What differences do you observe in the behaviour of the two algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the double-min-over learning rule\n",
    "def learn_dmo(samples, labels, epochs):\n",
    "\n",
    "    # TODO n_features: dimension of the feature vector\n",
    "    n_features =\n",
    "\n",
    "    # TODO: initialize weights (threshold not! included) randomly\n",
    "    weights = np.random.rand(n_features)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # TODO: extract training samples of class +1\n",
    "        samples_pos =\n",
    "    \n",
    "        # TODO: get x_min_pos\n",
    "        x_min_pos =\n",
    "\n",
    "        # TODO: extract training samples of class -1\n",
    "        samples_neg =\n",
    "\n",
    "        # TODO: get x_min_neg\n",
    "        x_min_neg =\n",
    "\n",
    "        # TODO: adapt weight vector, i.e. apply DMO learning rule\n",
    "        weights =\n",
    "\n",
    "        # TODO: calculate threshold\n",
    "        threshold =\n",
    "        \n",
    "        # yield weight vector and threshold\n",
    "        yield (weights, threshold)\n",
    "\n",
    "samples, labels = utils.load_data('data/data_2_2.npz')\n",
    "animation = utils.Animation(samples, labels)\n",
    "weights = list(learn_dmo(samples, labels, \n",
    "                         epochs=50))\n",
    "animation.play(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
